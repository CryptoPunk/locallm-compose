services:
  openai:
    extends: 
      file: ../include/vllm.yml
      service: vllm
    environment:
      VLLM_API_KEY: ${API_KEY}
    expose: [6900]
    command: >
        --port 6900
        --task generate
        --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B
        --quantization fp8
        --enforce-eager
        --load-format runai_streamer
        --tool-call-parser hermes
        --enable-auto-tool-choice